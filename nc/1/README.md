# Многослойный перцептрон с графическим интерфейсом

## Описание

Многослойный перцептрон - это тип искусственной нейронной сети прямого распространения, состоящий из нескольких слоев нейронов. Каждый слой полностью соединен со следующим слоем. Эта реализация включает в себя:

- Создание сети с произвольным количеством слоев и нейронов
- Обучение методом обратного распространения ошибки
- Предсказание на новых данных
- Графический интерфейс для удобного взаимодействия

## Интерфейс программы

### Главное окно

Программа открывается в окне размером 800x600 пикселей с четырьмя основными разделами:

#### 1. Настройка сети
- **Поле "Структура сети"**: Здесь вводится архитектура сети в формате чисел через запятую
  - Пример: `2,4,3,1` означает:
    - Входной слой: 2 нейрона
    - Первый скрытый слой: 4 нейрона
    - Второй скрытый слой: 3 нейрона
    - Выходной слой: 1 нейрон
- **Кнопка "Создать сеть"**: Инициализирует новую нейронную сеть с заданной архитектурой

#### 2. Параметры обучения
- **Поле "Эпохи"**: Количество итераций обучения (по умолчанию 1000)
- **Поле "Скорость обучения"**: Параметр, контролирующий размер шагов при обновлении весов (по умолчанию 0.1)
- **Кнопка "Обучить"**: Запускает процесс обучения сети

#### 3. Тестирование
- **Поле "Входные данные"**: Данные для предсказания в формате чисел через запятую
- **Кнопка "Предсказать"**: Выполняет предсказание для введенных данных

#### 4. Результаты
- **Текстовое поле с прокруткой**: Отображает все результаты работы программы:
  - Информацию о созданной сети
  - Прогресс обучения
  - Результаты предсказаний
  - Ошибки и предупреждения

## Как работает программа

### 1. Инициализация

При запуске программа:
- Создает графический интерфейс
- Генерирует тестовые данные состояния воды для демонстрации
- Выводит инструкции по использованию

### 2. Создание сети

Когда пользователь нажимает "Создать сеть":
- Парсится строка структуры сети
- Создается объект `MultilayerPerceptron`
- Инициализируются веса случайными значениями
- Инициализируются смещения нулевыми значениями
- Выводится информация о созданной сети

### 3. Обучение

Процесс обучения происходит в отдельном потоке, чтобы не блокировать интерфейс:

#### Прямое распространение (Forward Propagation)
1. Входные данные проходят через каждый слой
2. На каждом слое вычисляется: `выход = sigmoid(вход * веса + смещение)`
3. Выход одного слоя становится входом следующего

#### Обратное распространение (Backward Propagation)
1. Вычисляется ошибка на выходном слое: `ошибка = предсказание - цель`
2. Ошибка распространяется назад через все слои
3. Вычисляются градиенты для весов и смещений

#### Обновление параметров
1. Веса обновляются: `новый_вес = старый_вес - скорость_обучения * градиент`
2. Смещения обновляются аналогично

### 4. Предсказание

При нажатии "Предсказать":
- Входные данные преобразуются в массив NumPy
- Выполняется прямое распространение через обученную сеть
- Результат выводится в окно результатов

## Тестовые данные (Состояние воды)

По умолчанию программа использует задачу классификации состояния воды по температуре и давлению:

| Температура (°C) | Давление (атм) | Состояние | Выход |
|------------------|----------------|-----------|-------|
| -10              | 1.0            | Лед       | 0.0   |
| 0                | 1.0            | Почти лед | 0.1   |
| 25               | 1.0            | Вода      | 0.5   |
| 50               | 1.0            | Теплая вода| 0.6   |
| 75               | 1.0            | Горячая вода| 0.8  |
| 100              | 1.0            | Почти пар | 0.9   |
| 120              | 1.0            | Пар       | 1.0   |

Эта задача более практична и наглядна, чем классическая XOR. Она демонстрирует, как нейронная сеть может изучать физические закономерности фазовых переходов воды.

## Пример использования

### Шаг 1: Создание сети
1. В поле "Структура сети" введите: `2,4,1`
2. Нажмите "Создать сеть"
3. В окне результатов появится информация о созданной сети

### Шаг 2: Обучение
1. Оставьте параметры по умолчанию (1000 эпох, скорость 0.1)
2. Нажмите "Обучить"
3. Наблюдайте за процессом обучения в окне результатов

### Шаг 3: Тестирование
1. В поле "Входные данные" введите: `0.17,0.33` (25°C, 1 атм)
2. Нажмите "Предсказать"
3. Результат должен быть близок к 0.5 (вода при комнатной температуре)

## Технические детали

### Функция активации
Используется сигмоидная функция: `σ(x) = 1 / (1 + e^(-x))`
- Сжимает выход в диапазон (0, 1)
- Дифференцируема, что необходимо для обратного распространения

### Функция потерь
Используется среднеквадратичная ошибка: `MSE = (1/m) * Σ(y_pred - y_true)²`
- Простая в вычислении и дифференцировании
- Подходит для задач регрессии

### Инициализация весов
Веса инициализируются случайными значениями из нормального распределения, умноженными на 0.5:
- Предотвращает симметрию весов
- Обеспечивает хорошую начальную точку для обучения

## Возможные улучшения

1. **Дополнительные функции активации**: ReLU, tanh, softmax
2. **Регуляризация**: L1, L2, dropout
3. **Оптимизаторы**: Adam, RMSprop, momentum
4. **Визуализация**: графики потерь, архитектуры сети
5. **Сохранение/загрузка**: возможность сохранить обученную модель
6. **Пакетное обучение**: обработка данных батчами
7. **Валидация**: разделение данных на обучающую и валидационную выборки

## Структура кода

### Класс `MultilayerPerceptron`
- `__init__()`: Инициализация сети
- `sigmoid()`: Функция активации
- `forward_propagation()`: Прямое распространение
- `backward_propagation()`: Обратное распространение
- `train()`: Обучение сети
- `predict()`: Предсказание

### Класс `MLPInterface`
- `__init__()`: Создание интерфейса
- `create_widgets()`: Создание элементов GUI
- `create_network()`: Обработка создания сети
- `train_network()`: Обработка обучения
- `predict()`: Обработка предсказания
